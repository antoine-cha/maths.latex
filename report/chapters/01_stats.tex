%! TEX root = ../main.tex
\documentclass[../main.tex]{subfiles}
\begin{document}

% ============================================================ %
% ============================================================ %
\chapter{Statistics}
% ============================================================ %
% ============================================================ %
\section{Random sample}
A random sample is a collection of $n$ random variables $X_1,\dots, X_n$ that are independent and
identically distributed with $X$.


\section{Mean}
The mean is $\mu = E[X]$ . \\
The sample mean is $\bar{X} = \frac{1}{n} \sum_{i=1}^{n}{X_i}$ . \\
It is unbiased, i.e. $E[\bar{X}] = \mu$ .

\subsection{Central limit theorem}
Let us have a random sample $X_1,\dots, X_n$ following a mean $\mu$ and a variance $\sigma^2$, then we have
\begin{align*}
    &\bar{X} \underset{n \to +\infty}{\sim} \mathcal{N}(\mu, \frac{\sigma^2}{n}) \\
    &\frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}} \underset{n \to +\infty}{\sim} \mathcal{N}(0, 1)
\end{align*}

\subsection{Properties}
\begin{align*}
    \text{Linearity }&E[aX + bY] = aE[X] + b[Y]
\end{align*}

\subsection{Law of total expectaction}
\begin{align*}
    &E[X] = E[E[X|Y]]
\end{align*}

\section{Variance}
The variance is $Var[X] = E[(X - \bar{X})^2]$. \\
The sample variance is $s^2 = \hat{\sigma}^2 = \frac{1}{n-1} \sum_{i=1}^{n}{(X_i - \bar{X})^2}$ \\
It is unbiased, i.e. $E[\hat{\sigma}^2] = \sigma^2$.

\subsection{Properties}
\begin{align*}
    \text{Bilinearity }&Var[aX + bY] = a^2Var[X] + 2abCov[X, Y] + b^2 Var[Y]
\end{align*}

\subsection{Chi-squared relation with sample variance}
Let $s^2$ be the sample variance,
\begin{align*}
    \frac{s^2(n-1)}{\sigma^2} \sim \chi_{n-1}^2
\end{align*}

\section{Bias}
The bias of an estimator $\hat{\theta}$ of a parameter $\theta$ is
\begin{align*}
    Bias(\hat{\theta}) = E[\hat{\theta}] - \theta
\end{align*}
An estimator is said unbiased if $Bias(\hat{\theta}) = 0$ .




\subsection{Law of total variance}
Also called variance decomposition formula or conditional variance formulas or law of iterated variances
\begin{align*}
    Var(Y) = E[Var(Y | X)] + Var(E[Y | X])
\end{align*}

\section{Types of convergence}

\section{Law of large numbers (LLNs)}
Laws that specify conditions for which the mean estimator converges to a constant.
\subsection{Strong laws}
\subsubsection{Kolmogorov's strong LLNs}
Let $\{X_n\}$ be an \textbf{IID} sequence of random variables having finite mean $\mu < \infty$, then
\begin{align*}
    E[X_n] \xrightarrow{a. s.} \mu
\end{align*}

\subsubsection{Ergodic theorem}
Let $\{X_n\}$ be a \textbf{stationary} and \textbf{ergodic} sequence of random variables having finite mean $\mu < \infty$, then
\begin{align*}
    E[X_n] \xrightarrow{a. s.} \mu
\end{align*}

\subsection{Weak laws}
\subsubsection{Chebyshev's Weak LLNs - uncorrelated}
Let $\{X_n\}$ be an \textbf{uncorrelated} and \textbf{covariance stationary} sequence of random variables, that is
\begin{align*}
    &\exists\, \mu \in \mathbb{R}: E[X_n] = \mu, \forall n \in \mathbb{N} \\
    &\exists\, \sigma^2 \in \mathbb{R}: Var[X_n] = \sigma^2, \forall n \in \mathbb{N} \\
    & Cov[X_n, X_{n+k}] = 0, \forall n,k \in \mathbb{N}
\end{align*}
then a weak LLNs applies
\begin{align*}
    E[X_n] \xrightarrow{p.} \mu
\end{align*}

\subsubsection{Chebyshev's Weak LLNs - correlated}
Let $\{X_n\}$ be an \textbf{correlated} and \textbf{covariance stationary} sequence of random variables, that is
\begin{align*}
    &\exists\, \mu \in \mathbb{R}: E[X_n] = \mu, \forall n \in \mathbb{N} \\
    &\exists\, \sigma^2 \in \mathbb{R}: Var[X_n] = \sigma^2, \forall n \in \mathbb{N} \\
    & Cov[X_n, X_{n+k}] = \gamma_k, \forall n,k \in \mathbb{N}
\end{align*}
Then, \\
\begin{align*}
    &\text{if covariances tend to be 0 in average, that is }\underset{n \to \infty}{lim} \frac{1}{n} \sum_{i=1}^{n} \gamma_i = 0 \\
    &\text{then a weak LLNs applies } E[X_n] \xrightarrow{p.} \mu
\end{align*}

\subsection{For vectors}
The law(s) apply to all components $\Longleftrightarrow$ the law(s) apply the vector.

\section{Moment generating function}
For $X$ a random variable, if $E[e^{xT}]$ exists for $t \in [-h, h], h > 0$,
then $X$ has a moment generating function:
\begin{align*}
    &\text{Definition: }&M_X(t) = E[e^{tX}] \\
    &\text{Moments generation: }&\frac{d^nM_X(0)}{dt^n} = E[X^n] \\
\end{align*}
\subsection{Properties}
\begin{align*}
    &\text{Equality of distributions: }& F_X = F_Y \Longleftrightarrow M_X = M_Y \\
    &\text{Linear transformation: }& Y = a + bX, \, M_Y(t) = e^{at}M_X(bt) \\
    &\text{Sum of mutually independent variables:}& Z =\sum{X_i}, M_Z(t) = \prod{M_{X_i}(t)}
\end{align*}
\subsection{Multivariate}
Use a vector $t$ and the formula becomes $M_X(t) = E[e^{t^TX}]$. \\
By differentiating $n_1, \dots, n_N$ times we can obtain the cross-moments.

\section{Characteristic function}
For $X$ a random variable, the characteristic function always exists.
\begin{align*}
    &\text{Definition: }&\varphi_X(t) = E[e^{itX}] \\
\end{align*}
\subsection{Deriving moments}
If $E[X^n]$ exists and is finite, then
\begin{align*}
    &\varphi_X(t)\text{is n-times continuously differentiable}\\
    &\text{and }\mu_X(n) = E[X^n] = \frac{1}{i^n}\frac{d^n\varphi_X(0)}{dt^n}  \\
\end{align*}
Knowing whether a moment exists is unusual, so a more useful version is: \\
\\
if $\varphi_X(t)$ is n-times differentiable in 0, then
\begin{itemize}
    \item if $n$ is \textbf{even}, $\forall k \leq n, E[X^n]\text{ exists and } E[X^n] = \frac{1}{i^n}\frac{d^n\varphi_X(0)}{dt^n} $
    \item if $n$ is \textbf{odd}, $\forall k < n, E[X^n]\text{ exists and } E[X^n] = \frac{1}{i^n}\frac{d^n\varphi_X(0)}{dt^n} $
\end{itemize}
\subsection{Properties}
\begin{align*}
    &\text{Equality of distributions: }& F_X = F_Y \Longleftrightarrow \varphi_X = \varphi_Y \\
    &\text{Linear transformation: }& Y = a + bX, \, \varphi_Y(t) = e^{iat}\varphi_X(bt) \\
    &\text{Sum of mutually independent variables:}& Z =\sum{X_i}, \varphi_Z(t) = \prod{\varphi_{X_i}(t)}
\end{align*}


\section{Common distributions}
\subsection{Discrete}
\begin{center}
    \begin{tabular}{|c |c |c |c |c|}
        \hline
        Name & Density & Mean & Variance & Mom gen func \\
        \hline
        Uniform - $U(a, b)$& $\frac{1}{b-a}$ & $\frac{a+b}{2}$ & $\frac{(b-a)^2}{12}$ & $m(t)=\frac{e^{bt} - e^{at}}{t(b-a)}$ \\
        \hline
        Bernoulli - $B(p)$ & $f(0)=1-p, f(1)=p$ & $p$ & $p(1-p)$ & $m(t)=pe^t(1-p)$ \\
        \hline
        Binomial(n, p) & $f(x) = \binom{n}{x} p^x(1-p^{n-x})$ & $np$ & $np(1-p)$ & $m(t) = (pe^t + 1-p)^n$\\
        \hline
    \end{tabular}
\end{center}

\subsection{Continuous}
\begin{center}
    \begin{tabular}{|c |c |c |c |c|}
        \hline
        Name & Density & Mean & Variance & Mom gen func \\
        \hline
        Uniform & \\
        \hline
        Bernoulli & \\
        \hline
        Binomial & \\
        \hline
        Binomial & \\
    \end{tabular}
\end{center}


\end{document}
