%! TEX root = ../main.tex
\documentclass[../main.tex]{subfiles}
\begin{document}

% ============================================================ %
% ============================================================ %
\chapter{Deep learning}
% ============================================================ %
% ============================================================ %

\section{Generative Adversarial Networks}
These systems train a generative $G$ network and a discriminator $D$ network. $G$ learns to generate
samples from inputs (noise and potentially more). $D$ learns to discriminate between real samples and
samples generated by $G$.

\subsection{Basic formula}
We can define the generator $G$ and the discriminator $D$ as:
\begin{align*}
    &G: \mathcal{Z} \to \mathcal{X} \\
    &D: \mathcal{X} \to [0. 1] \\
\end{align*}
where $\mathcal{Z}$ is the latent space and $\mathcal{X}$ is the sample space. The functions are
parameterized by respectively $\Theta_G$ and $\Theta_D$. \\
The loss function is
\begin{align*}
    &\mathcal{L}_{GAN}(G, D) = \mathbb{E}_{x,y}[log(D(y))] + \mathbb{E}_{x,z}[log(1-D(x, G(x, z)))] \\
    &\text{with} \\
    & x\quad \text{the observed input} \\
    & y\quad \text{the true output} \\
    & z\quad \text{the noise vector} \\
\end{align*} \\
$D$ and $G$ are competing so the training objective is
\begin{equation}
    \min_G \max_D \mathcal{L}_{GAN}(G, D)
\end{equation}

Compared to more traditional approaches like PCA, ICA, Fourier or wavelet representations, GANs allow
for a higher level of complexity. Thus, mappings defined by DNNs can be extraordinarily complex. \\

\section{GAN upgrades}
\subsection{DCGAN}
Architectures improvements for GANs:
\begin{itemize}
    \item Replace pooling layers with strided convs (discriminator)
    \item Use deconvolution (fractionally-strided convolutions)
    \item Use BN in generator and discriminator
    \item Remove fully connected layers
    \item Use ReLU activation in generator except output layers which uses $tanh$
    \item Use LeakyReLU in the discriminator for all layers
\end{itemize}

\subsection{Laplacian GAN (LAPGAN)}
Let $I$ be the input image,\\
Let $d$ be a downsampling operator that blurs then decimates the image by a factor of 2, \\
Let $u$ be an upsampling operator that smooths and expands I by a factor of 2, \\
\begin{align*}
    &\mathcal{G}(I) = [I_0, ..., I_K]  \quad \text{is a Gaussian pyramid where} \\
    &I_0 = I \\
    &I_k = \mathcal{G}_k = d^k(I) \quad \text{($k$ repeated applications)}\\
    \\
    &\mathcal{L}(I) = [h_1, ..., h_K]  \quad \text{is the Laplacian pyramid where} \\
    &h_K = I_K \\
    &h_k = \mathcal{L}_k = \mathcal{G}_k - u(\mathcal{G}_{k+1}(I)) = I_k - u(I_{k+1}) \\
\end{align*}
We can reconstruct the image from a Laplacian pyramid coefficients $[h_1, ..., h_K]$ using
the backward recurrence
\begin{equation}
    I_k = u(I_{k+1}) + h_k
\end{equation}
which is started with $I_K = h_K$ and finished with $I = I_0$.


\subsection{Cycle GAN}
\subsection{WGAN}

\subsection{Style GAN}



\section{pix2pix}
From the paper "Image-to-Image Translation with Conditional Adversarial Networks" \cite{pix-to-pix}.
Uses conditional GANs to create images from images (image translation).


\subsection{Introduction}
At the core of image translation is the capability to judge the quality of a translation. By using a GAN,
we can learn the loss function as well as the mapping. \\
L2 loss is minimized by averaging all plausible outputs, causing blurry outputs, so they prefer L1.
Note that L1 loss is minimized by selecting the median. \\
Generator is U-net, discriminator is PatchNet.

\subsection{Method}
Conditional GANs learn a mapping $G: \{x, z\} \rightarrow y$ where $x$ is the observed input and $z$
is a random noise vector. \\
\textbf{TODO} understand why conditioning the discriminator is important. \\
The objective of a cGAN is
\begin{equation}
    \mathcal{L}_{cGAN}(G, D) = \mathbb{E}_{x,y}[log(D(x,y))] + \mathbb{E}_{x,z}[log(1-D(x, G(x, z)))]
\end{equation}
where $G$ tries to minimize against $D$ that tries to maximize it, i.e.
\begin{equation}
    G^* = \arg \min_G \max_D \mathcal{L}_{cGAN}(G, D)
\end{equation}
The non-conditional GAN is when the discriminator does not observe $x$
\begin{equation}
    \mathcal{L}_{GAN}(G, D) = \mathbb{E}_{x,y}[log(D(y))] + \mathbb{E}_{x,z}[log(1-D(x, G(x, z)))]
\end{equation}

When introducing L1
\begin{align}
    &\mathcal{L}_{L1}(G) = \mathbb{E}_{x,y,z}[\|y - G(x, z)\|_1] \\
    &\mathcal{L}_{pix2pix}(G, D) = \mathcal{L}_{cGAN}(G, D) + \lambda \mathcal{L}_{L1}(G)
\end{align}


\section{PixelCNN}
\section{Autoencoders, VAE, Adversarial Variational Bayes (AVB)}
\section{Optimizers: SGD. momentum, Nesterov momentum, RMSProp, Adam}


\end{document}
