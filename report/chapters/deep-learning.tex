%! TEX root = ../main.tex
\documentclass[../main.tex]{subfiles}
\begin{document}

% ============================================================ %
% ============================================================ %
\chapter{Deep learning}
% ============================================================ %
% ============================================================ %

\section{Generative Adversarial Networks}
These systems train a generative $G$ network and a discriminator $D$ network. $G$ learns to generate
samples from inputs (noise and potentially more). $D$ learns to discriminate between real samples and
samples generated by $G$.

\subsection{Basic formula}
We can define the generator $G$ and the discriminator $D$ as:
\begin{align*}
    &G: \mathcal{Z} \to \mathcal{X} \\
    &D: \mathcal{X} \to [0. 1] \\
\end{align*}
where $\mathcal{Z}$ is the latent space and $\mathcal{X}$ is the sample space. The functions are
parameterized by respectively $\Theta_G$ and $\Theta_D$. \\
The loss function is
\begin{align*}
    &\mathcal{L}_{GAN}(G, D) = \mathbb{E}_{x,y}[log(D(y))] + \mathbb{E}_{x,z}[log(1-D(x, G(x, z)))] \\
    &\text{with} \\
    & x\quad \text{the observed input} \\
    & y\quad \text{the true output} \\
    & z\quad \text{the noise vector} \\
\end{align*} \\
$D$ and $G$ are competing so the training objective is
\begin{equation}
    \min_G \max_D \mathcal{L}_{GAN}(G, D)
\end{equation}

Compared to more traditional approaches like PCA, ICA, Fourier or wavelet representations, GANs allow
for a higher level of complexity. Thus, mappings defined by DNNs can be extraordinarily complex.



\section{pix2pix}
From the paper "Image-to-Image Translation with Conditional Adversarial Networks" \cite{pix-to-pix}.
Uses conditional GANs to create images from images (image translation).


\subsection{Introduction}
At the core of image translation is the capability to judge the quality of a translation. By using a GAN,
we can learn the loss function as well as the mapping. \\
L2 loss is minimized by averaging all plausible outputs, causing blurry outputs, so they prefer L1.
Note that L1 loss is minimized by selecting the median. \\
Generator is U-net, discriminator is PatchNet.

\subsection{Method}
Conditional GANs learn a mapping $G: \{x, z\} \rightarrow y$ where $x$ is the observed input and $z$
is a random noise vector. \\
\textbf{TODO} understand why conditioning the discriminator is important. \\
The objective of a cGAN is
\begin{equation}
    \mathcal{L}_{cGAN}(G, D) = \mathbb{E}_{x,y}[log(D(x,y))] + \mathbb{E}_{x,z}[log(1-D(x, G(x, z)))]
\end{equation}
where $G$ tries to minimize against $D$ that tries to maximize it, i.e.
\begin{equation}
    G^* = \arg \min_G \max_D \mathcal{L}_{cGAN}(G, D)
\end{equation}
The non-conditional GAN is when the discriminator does not observe $x$
\begin{equation}
    \mathcal{L}_{GAN}(G, D) = \mathbb{E}_{x,y}[log(D(y))] + \mathbb{E}_{x,z}[log(1-D(x, G(x, z)))]
\end{equation}

When introducing L1
\begin{align}
    &\mathcal{L}_{L1}(G) = \mathbb{E}_{x,y,z}[\|y - G(x, z)\|_1] \\
    &\mathcal{L}_{pix2pix}(G, D) = \mathcal{L}_{cGAN}(G, D) + \lambda \mathcal{L}_{L1}(G)
\end{align}


\section{PixelCNN}
\section{Autoencoders, VAE, Adversarial Variational Bayes (AVB)}
\section{Optimizers: SGD. momentum, Nesterov momentum, RMSProp, Adam}


\end{document}
