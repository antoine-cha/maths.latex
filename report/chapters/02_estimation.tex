
%! TEX root = ../main.tex
\documentclass[../main.tex]{subfiles}
\begin{document}

% ============================================================ %
% ============================================================ %
\chapter{Estimation}
% ============================================================ %
% ============================================================ %
\section{Definitions}
The observed data is $D = (X_1, \dots, X_n)$ . \\
The posterior is $P(\theta\, | \, D)$ . \\
The prior is $P(\theta)$ . \\
The likelihood is $P(D\; |\; \theta)$ . \\
Using Bayes rule $P(\theta | D) = \frac{P(D|\theta)P(\theta)}{P(D)}$,
so $P(\theta | D) \propto P(D|\theta)P(\theta)$ .

\subsubsection{Conjugate prior}
If the prior and the posterior are from the same distribution family,
then the prior and posterior are called \textbf{conjugate distributions}. \\
The prior is called a \textbf{conjugate prior}. \\
A conjugate prior helps with modeling as:
\begin{itemize}
    \item it leads to a closed-form expression for the posterior distribution.
    \item it is easy to interpret, we can easily see how the parameters of the prior change after the Bayesian update.
\end{itemize}

\subsubsection{Maximum likelhood estimation (MLE)}
The maximum likelihood estimation chooses $\theta$ that maximimizes the probability of observed data:
\begin{align*}
    \hat{\theta}_{MLE} = arg\,\underset{\theta}{max}\; P(D\,|\, \theta)
\end{align*}

\subsubsection{Maximum a posteriori estimation (MAP)}
The MAP estimation chooses $\theta$ that is most probable given observed data $D$ and prior belief
$P(\theta)$.

\begin{align*}
    \hat{\theta}_{MAP} & = arg\,\underset{\theta}{max}\; P(\theta | D)           \\
                       & = arg\,\underset{\theta}{max}\; P(D | \theta) P(\theta) \\
\end{align*}

\section{Bias-Variance trade-off}
We want to learn a function $h$ such that $y = h(x)$.
Let
\begin{itemize}
    \item $\bar{y}(x) = E_{y|x}[y]$, there is always some unknown on whether we can perfectly model $y$ from $x$
    \item $D$ be a set of pairs $\{(x_1,y_1),\dots,(x_n,y_n)\}$ sampled from $P(x, y)$
    \item $\mathcal{A}$ be a learning algorithm such that $h_D = \mathcal{A}(D)$ and $\bar{h} = E_D[h_D]$
\end{itemize}
The expected mean-squared error of our learning algorithm:

\begin{align*}
    E_{x,y,D}[(y - h_D(x))^2] =
    {\underbrace{E_{x}[(\bar{h}(x) - \bar{y}(x))^2]}_{\mathclap{Bias^2}}} + \\
    {\underbrace{E_{x,D}[(h_D(x) - \bar{h}(x))^2]}_{\mathclap{Variance}}} +
    {\underbrace{E_{x,y}[(y - \bar{y}(x))^2]}_{\mathclap{Noise}}}
\end{align*}

\section{Markov chains}

A \textbf{stochastic process} $X = \{X(t): t \in T\}$ is a collection of random variables. $X(t)$ is
called the \textbf{state} of the process at $t$.

A discrete time process $X = \{X(t): t \in \mathbb{N}\}$ is called a \textbf{Markov chain}
$\Leftrightarrow$
\begin{align*}
     & P(X_t=a_T | X_{t-1} = a_{t-1}, ..., X_0 = a_0) = P(X_t = a_t | X_{t-1} = a_{t-1}) \\
     & \forall t \in \mathbb{N}^*, \quad \forall a_i \in A                               \\
\end{align*}
A Markov chain is called \textbf{homogeneous} $\Leftrightarrow$ its transition probabilities are
independent of $t$
\begin{align*}
    P_{i,j} = P(X_t = a_j | X_{t-1} = a_i)
\end{align*}
A Markov chain is called \textbf{finite} $\Leftrightarrow$ its set of values is a finite set.
In such a case, we can determine the transition matrix
\begin{align*}
    P = (P_{i, j}) =
    \begin{bmatrix}
        P_{0,0}  & P_{0,1} & ... \\
        P_{1, 0} & P_{1,1} & ... \\
        ...      & ...     & ... \\
    \end{bmatrix}
\end{align*}

We define the \textbf{m-step transition probability} as
\begin{align*}
     & P^n_{i,j} = P(X_{t+n} = a_j | X_{t} = a_i) \\
     & (P^n_{i, j}) = P^n
\end{align*}

\section{Stationarity}
Let $\{ X_t \}$ be a stochastic process, \\
let $F_X(x_{t_1}, ..., x_{t_n})$ represent the cumulative distribution function \\
$\{ X_t \}$ is said to be \textbf{strictly stationary, strongly stationary or strict-sense stationary if}
\begin{align*}
     & F_X(x_{t_1+\tau}, ..., x_{t_n+\tau}) = F_X(x_{t_1}, ..., x_{t_n})    \\
     & \forall \tau, t_1, ..., t_n \in \mathbb{R}, \forall n \in \mathbb{N}
\end{align*}

\section{Karush-Kuhn-Tucher (KKT) conditions}


\section{PCA, ICA}
\subsection{Principal Component Analysis}
In PCA, the basis functions emerge as the eigenvectors of the covariance matrix over observations of
the input data. So the mapping is linear and shallow.

\subsection{Independent Component Analysis}
Can use Noise Contrastive Estimation
F

\end{document}
