%! TEX root = ../main.tex
\documentclass[../main.tex]{subfiles}
\begin{document}

% ============================================================ %
% ============================================================ %
\chapter{Generative models}
% ============================================================ %
% ============================================================ %

\section{Generative Adversarial Networks}
These systems train a generative $G$ network and a discriminator $D$ network. $G$ learns to generate
samples from inputs (noise and potentially more). $D$ learns to discriminate between real samples and
samples generated by $G$.

\subsection{Basic formula}
We can define the generator $G$ and the discriminator $D$ as:
\begin{align*}
    &G: \mathcal{Z} \to \mathcal{X} \\
    &D: \mathcal{X} \to [0. 1] \\
\end{align*}
where $\mathcal{Z}$ is the latent space and $\mathcal{X}$ is the sample space. The functions are
parameterized by respectively $\Theta_G$ and $\Theta_D$. \\
The loss function is
\begin{align*}
    &\mathcal{L}_{GAN}(G, D) = \mathbb{E}_{x,y}[log(D(y))] + \mathbb{E}_{x,z}[log(1-D(x, G(x, z)))] \\
    &\text{with} \\
    & x\quad \text{the observed input} \\
    & y\quad \text{the true output} \\
    & z\quad \text{the noise vector} \\
\end{align*} \\
$D$ and $G$ are competing so the training objective is
\begin{equation}
    \min_G \max_D \mathcal{L}_{GAN}(G, D)
\end{equation}

Compared to more traditional approaches like PCA, ICA, Fourier or wavelet representations, GANs allow
for a higher level of complexity. Thus, mappings defined by DNNs can be extraordinarily complex. \\

\section{GAN upgrades}
\subsection{DCGAN}
Architectures improvements for GANs:
\begin{itemize}
    \item Replace pooling layers with strided convs (discriminator)
    \item Use deconvolution (fractionally-strided convolutions)
    \item Use BN in generator and discriminator
    \item Remove fully connected layers
    \item Use ReLU activation in generator except output layers which uses $tanh$
    \item Use LeakyReLU in the discriminator for all layers
\end{itemize}

\subsection{Laplacian GAN (LAPGAN)}
Let $I$ be the input image,\\
Let $d$ be a downsampling operator that blurs then decimates the image by a factor of 2, \\
Let $u$ be an upsampling operator that smooths and expands I by a factor of 2, \\
\begin{align*}
    &\mathcal{G}(I) = [I_0, ..., I_K]  \quad \text{is a Gaussian pyramid where} \\
    &I_0 = I \\
    &I_k = \mathcal{G}_k = d^k(I) \quad \text{($k$ repeated applications)}\\
    \\
    &\mathcal{L}(I) = [h_1, ..., h_K]  \quad \text{is the Laplacian pyramid where} \\
    &h_K = I_K \\
    &h_k = \mathcal{L}_k = \mathcal{G}_k - u(\mathcal{G}_{k+1}(I)) = I_k - u(I_{k+1}) \\
\end{align*}
We can reconstruct the image from a Laplacian pyramid coefficients $[h_1, ..., h_K]$ using
the backward recurrence
\begin{equation}
    I_k = u(I_{k+1}) + h_k
\end{equation}
which is started with $I_K = h_K$ and finished with $I = I_0$.

\subsection{Metrics for quality of generated samples}
\subsubsection{Human preference}
Have human selects the best translation out of 2.

\subsubsection{Inception score (translation accuracy, photrealism)}
Accuracy of Inception

\subsubsection{Fr√©chet Inception score (distribution matching)}
The Wasserstein metric between 2 multidimensional Gaussian distributions:
\begin{itemize}
    \item $\mathcal{N}(\mu, \Sigma)$, ditribution of Inception features on generated images
    \item $\mathcal{N}(\mu_w, \Sigma_w)$, ditribution of Inception features on real images
\end{itemize}
\begin{equation}
    FID = |\mu - \mu_w|^2 + tr(\Sigma + \Sigma_w - 2 (\Sigma\Sigma_w)^{1/2})
\end{equation}

\subsubsection{Learned Perceptual Image Path Similarity}
introduced in \cite{unreasonable-deep-metric}. Simply learn a linear layer on top of pre-trained
model. Distance is computed as the $l_2$ distance between normalized then scaled activations. \\
It has strong correlation with human judgment.

\subsubsection{Domain-invariant perceptual distance (DPID)}
$l_2$ distance between 2 normalized VGG Conv5 features, which is more invariant against domain change.

\subsection{Few-shot unsupervised Image-to-Image Translation (FUNIT)}
Let, \\
$x$ be the content image of class $c_x$, \\
$\{y_1, ...\ y_K\}$ a set of target class images of class $c_y$, \\
\\
the generated image is $\bar{x} = G(x, \{y_1, ...\ y_K\})$ \\
the content encoder $E_x$ encodes the content as $E_x(x)$ \\
the class encoder $E_y$ encodes the target class as $E_y(\{y_1, ...\ y_K\})$ \\
the decoder $F_x$ generates the image as $\bar{x} = F_x(E_x(x), E_y(\{y_1, ...\ y_K\}))$ \\
\\
The optimization problem of FUNIT is
\begin{equation}
    \min_G\max_D \mathcal{L}_{GAN}(D, G) + \lambda_R\mathcal{L}_R(G) + \lambda_F\mathcal{L}_{FM}(G)
\end{equation}
The discriminator is learnt multi-task, it predicts for each class whether the input is real image of
the class. For a real image of class $c_x$ its $c_x$th output must
be 1. For a generated image with target class $c_y$, the discrimator aims at $c_y$th  output being
0. \\
\begin{equation}
    \mathcal{L}_{GAN}(D, G) = E_x[log D^{c_x}(x)] + E_{x, \{y_1, ...\ y_K\}}[log(1-D^{c_y}(\bar{x}))]
\end{equation}

\paragraph{Content reconstruction loss:} helps G learn a translation model. When using the same
input content image and target class image, G should generate the input image.
\begin{equation}
    \mathcal{L}_R(G) = E_x[||x - G(x, \{x\})||_1]
\end{equation}
\paragraph{Feature Matching loss:} regularizes the training. We construct a feature extractor $D_f$
as D minus its last layer.
\begin{equation}
    \mathcal{L}_{FM}(G) = E_{x, \{y_1, ...\ y_K\}}
        \left[ \big\| D_f(\bar{x}) - \sum_k{\frac{D_f(y_k)}{K}} \big\|_1 \right]
\end{equation}

\subsection{Adaptative Instance Normalization (AdaIN)}
For style transfer, the idea is to have the content feature stats match the style feature statistics
\begin{equation}
    AdaIn(x_c, x_s) = \sigma(x_s) \left( \frac{x - \mu(x_c)}{\sigma(x_c)} \right) + \mu(x_s)
\end{equation}
AdaIN layer is only applied at the end of encoding when both style and content have been encoded.


\subsection{Cycle GAN (2017)}
Compared to Pix2Pix, Cycle GAN removes the need for paired inputs. The idea is to do 2-way generation:
$G: X \to Y$ and $F: Y \to X$. These are associated with 2 discriminators: $D_X$ and $D_Y$.
The new consistency loss with $L1$ is
\begin{equation}
    \mathcal{L}_{cyc}(G, F) = E_{X\sim p_{data}(x)} \left[ || F(G(x)) - x ||_1\right]
        + E_{y\sim p_{data}(y)} \left[ || G(F(y)) - y ||_1\right]
\end{equation}

\subsection{Star GAN (2017)}
Image-to-image translation for multiple domains (e.g. identity and emotion) using a single network.
This allows simultaneous training over multiple datasets with different domains.

\subsection{WGAN}

\subsection{Style GAN}


\section{pix2pix (2016)}
From the paper "Image-to-Image Translation with Conditional Adversarial Networks" \cite{pix-to-pix}.
Uses conditional GANs to create images from images (image translation).


\subsection{Introduction}
At the core of image translation is the capability to judge the quality of a translation. By using a GAN,
we can learn the loss function as well as the mapping. \\
L2 loss is minimized by averaging all plausible outputs, causing blurry outputs, so they prefer L1.
Note that L1 loss is minimized by selecting the median. \\
Generator is U-net, discriminator is PatchNet.

\subsection{Method}
Conditional GANs learn a mapping $G: \{x, z\} \rightarrow y$ where $x$ is the observed input and $z$
is a random noise vector. \\
Conditioning the discriminator enforces that the generate sample is generated with a clear target.
That it is not "just" a real-looking sample, but a \textbf{relevant} sample. \\
The objective of a cGAN is
\begin{equation}
    \mathcal{L}_{cGAN}(G, D) = \mathbb{E}_{x,y}[log(D(x,y))] + \mathbb{E}_{x,z}[log(1-D(x, G(x, z)))]
\end{equation}
where $G$ tries to minimize against $D$ that tries to maximize it, i.e.
\begin{equation}
    G^* = \arg \min_G \max_D \mathcal{L}_{cGAN}(G, D)
\end{equation}
The non-conditional GAN is when the discriminator does not observe $x$
\begin{equation}
    \mathcal{L}_{GAN}(G, D) = \mathbb{E}_{x,y}[log(D(y))] + \mathbb{E}_{x,z}[log(1-D(x, G(x, z)))]
\end{equation}

When introducing L1
\begin{align}
    &\mathcal{L}_{L1}(G) = \mathbb{E}_{x,y,z}[\|y - G(x, z)\|_1] \\
    &\mathcal{L}_{pix2pix}(G, D) = \mathcal{L}_{cGAN}(G, D) + \lambda \mathcal{L}_{L1}(G)
\end{align}


\section{PixelCNN}
\section{Autoencoders, VAE, Adversarial Variational Bayes (AVB)}
\section{Optimizers: SGD. momentum, Nesterov momentum, RMSProp, Adam}

\section{PixelCNN}


\end{document}
